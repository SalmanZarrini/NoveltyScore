{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from known import find_known_results\n",
    "import py2neo\n",
    "import normalize_nodes\n",
    "from lxml import etree\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from ars import retrieve_ars_results\n",
    "\n",
    "utf8_parser = etree.XMLParser(encoding='utf-8')\n",
    "\n",
    "def run_score(pk):\n",
    "    results = retrieve_ars_results(pk)\n",
    "    for ars, response in results.items():\n",
    "        known, unknown = find_known_results(response)\n",
    "#        nn_distance = calculate_nn_distance(known, unknown, response)\n",
    "        recency = calculate_recency(known, response,verbose=True)\n",
    "#run_score(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPublistForPairs(disease_idx,drug_identifier_list):\n",
    "    pubs_for_pairs = {}\n",
    "    #queryROBOKOPForPMIDsDiseaseDrugList\n",
    "    all_pubs = set()\n",
    "    normalized_drug_dict = normalize_nodes.normalize_big_list(drug_identifier_list)\n",
    "    normalized_drug_idxs = set()\n",
    "    for drug_key in drug_identifier_list:\n",
    "        if(drug_key not in normalized_drug_dict or normalized_drug_dict[drug_key]==None): drug_idx = drug_key\n",
    "        else: normalized_drug_idxs.add(normalized_drug_dict[drug_key][0])\n",
    "        #Drug_dict[drug_key]==None means the identifier can't be normalized. We won't be able to \n",
    "        #find it in ROBOKOP either.\n",
    "        #Otherwise we want to set the queried identifier to the normalized identifier.\n",
    "            \n",
    "    publications_from_rk = queryROBOKOPForPMIDsDiseaseDrugList(disease_idx,list(normalized_drug_idxs))\n",
    "    \n",
    "    #Denormalize the drug and make a dictonary for the publication.\n",
    "    denormed_publications = {}\n",
    "    for drug_key in drug_identifier_list:\n",
    "        if(drug_key not in normalized_drug_dict or normalized_drug_dict[drug_key]==None): \n",
    "            denormed_publications[(disease_idx,drug_key)] = []\n",
    "        else: \n",
    "            drug_idx = normalized_drug_dict[drug_key][0]\n",
    "            denormed_publications[(disease_idx,drug_key)] = publications_from_rk[(disease_idx,drug_key)]\n",
    "        all_pubs.update(publications_from_rk[(disease_idx,drug_key)])\n",
    "    return denormed_publications, all_pubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQueryNode(json_data):\n",
    "    query_node = json_data['message']['query_graph']['nodes']\n",
    "\n",
    "    if 'on' in query_node:\n",
    "    #Weird gene query\n",
    "        if query_node['on'].get('categories',[]) == ['biolink:Gene']:\n",
    "            query_idx = \"biolink:Gene\"\n",
    "            disease = False\n",
    "        elif('ids' in query_node['on']):\n",
    "            query_idx = query_node['on']['ids'][0]\n",
    "            disease = True\n",
    "        else:\n",
    "            query_idx = \"N/A\"\n",
    "            disease = False\n",
    "    elif 'n0' in query_node:\n",
    "        if('ids' in query_node['n0']):\n",
    "            query_idx = query_node['n0']['ids'][0]\n",
    "            disease = True\n",
    "        elif('id' in query_node['n1']):\n",
    "            query_idx = query_node['n1']['id']\n",
    "            disease = True\n",
    "        else:\n",
    "            query_idx = \"N/A\"\n",
    "            disease = True\n",
    "    elif 'disease' in query_node:\n",
    "        query_idx = query_node['disease']['ids'][0]\n",
    "        disease = True\n",
    "    #Could not find any node\n",
    "    else:\n",
    "        query_idx = \"N/A\"\n",
    "        disease=False\n",
    "    return (query_idx,disease)\n",
    "            \n",
    "\n",
    "def drug_idx_generator(json_data):\n",
    "    for result in json_data['message'].get('results',[]):\n",
    "        node_bindings = result['node_bindings']\n",
    "        if 'sn' in node_bindings:\n",
    "            drug_idx = node_bindings['sn'][0]['id']\n",
    "        elif 'n1' in node_bindings:\n",
    "            drug_idx = node_bindings['n1'][0]['id']\n",
    "        elif 'drug' in node_bindings:\n",
    "            drug_idx = node_bindings['drug'][0]['id']\n",
    "        elif \"chemical\" in node_bindings:\n",
    "            drug_idx = node_bindings['chemical'][0]['id']\n",
    "        else:\n",
    "            print(node_bindings)\n",
    "            raise Exception(\"Could not get Drug\")\n",
    "        \n",
    "        #NORMALIZE DRUG IDENTIFIER\n",
    "        #drug_idx = m[drug_idx]\n",
    "        yield drug_idx\n",
    "    return\n",
    "    '''        if drug_idx not in output_data[disease_idx]:\n",
    "            output_data[disease_idx][drug_idx] = [disease_idx, drug_idx, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "        #Noticed some Aragorn queries did not have scores in the results\n",
    "        if 'score' not in result:\n",
    "            return \"error\"\n",
    "        score = result['score']\n",
    "        if 'normalized_score' in result:\n",
    "            normalized_score = result['normalized_score']\n",
    "        else:\n",
    "            normalized_score = 0\n",
    "        output_data[disease_idx][drug_idx][ara_indexing[ara]] = score\n",
    "        output_data[disease_idx][drug_idx][ara_indexing[ara] + 1] = normalized_score\n",
    "    return \"success\"'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryROBOKOPForPMIDsDiseaseDrugList(disease_idx,drug_list):\n",
    "    '''\n",
    "    This function uses Py2Neo to query the ROBOKOPKG for each TextMiningKP edge for each pair\n",
    "    in the disease and drug list we found in the graph.\n",
    "    '''\n",
    "    graph = py2neo.Graph(host=\"robokopkg.renci.org\")\n",
    "    query_template = 'MATCH (d:`biolink:Disease`)-[r]-(c:`biolink:ChemicalEntity`) WHERE d.id=$disease_idx AND c.id in $drug_list AND r.`biolink:primary_knowledge_source`=\"infores:textminingkp\" RETURN c.id, r.publications'\n",
    "    publications_for_pair = defaultdict(set)\n",
    "    query_res = graph.run(query_template,parameters={\"disease_idx\":disease_idx, \"drug_list\":drug_list})\n",
    "    for message in query_res:\n",
    "#        for publication in message['r.publications']:\n",
    "        c_idx = message['c.id']\n",
    "        pub_list = message['r.publications']\n",
    "        publications_for_pair[(disease_idx,c_idx)].update(pub_list)\n",
    "    return publications_for_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PMCIDsToYear(pmcids):\n",
    "    time.sleep(0.2)\n",
    "    pmc_api_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=\" + ','.join(pmcids)\n",
    "    #print(pmc_api_url)\n",
    "    time.sleep(0.2)\n",
    "    r = requests.get(pmc_api_url)\n",
    "    s = r.text.encode('utf-8')\n",
    "    article_to_year = {}\n",
    "    if(len(s)>250):\n",
    "        tree = etree.fromstring(s, parser=utf8_parser)\n",
    "        for meta_ele in tree.xpath(\"//article-meta\"):\n",
    "            pmc_idx = \"PMC\" + meta_ele.xpath(\"./article-id[@pub-id-type='pmc']/text()\")[0]\n",
    "            #ub-date pub-type=\"epub\">\n",
    "            pub_year_list = meta_ele.xpath(\"./pub-date/year/text()\")\n",
    "            earliest_year = min([int(x) for x in pub_year_list])\n",
    "            article_to_year[pmc_idx] = earliest_year\n",
    "    return article_to_year\n",
    "\n",
    "def PMIDsToYear(pmids):\n",
    "    time.sleep(0.2)\n",
    "    pm_api_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=\" + ','.join(pmids)\n",
    "    r = requests.get(pm_api_url)\n",
    "    s = r.text.encode('utf-8')\n",
    "    article_to_year = {}\n",
    "    if(len(s)>250):\n",
    "        tree = etree.fromstring(s, parser=utf8_parser)\n",
    "        for article_ele in tree.xpath(\"//PubmedArticle\"):\n",
    "            pm_idx = \"PMID:\" + article_ele.xpath(\"./MedlineCitation/PMID/text()\")[0]\n",
    "            pub_year_list = article_ele.xpath(\"./PubmedData/History/PubMedPubDate/Year/text()\")\n",
    "            earliest_year = min([int(x) for x in pub_year_list])\n",
    "            article_to_year[pm_idx] = earliest_year\n",
    "    return article_to_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a9429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(l):\n",
    "    for i in range(0,len(l),250):\n",
    "        lower = i\n",
    "        upper = min(i+250,len(l))\n",
    "        yield l[lower:upper]\n",
    "\n",
    "def getDatesFromPaperIdentifiers(paper_ids):\n",
    "    paper_idx_to_date = {}\n",
    "    pubmed_ids = [x for x in paper_ids if \"PMC\" not in x]\n",
    "    #Break our queries into chunks of length 250.\n",
    "    for chunk in chunk_list(pubmed_ids):\n",
    "        paper_idx_to_date.update(PMIDsToYear(chunk))\n",
    "    \n",
    "    pmc_ids = [x for x in paper_ids if \"PMC\" in x]\n",
    "    for chunk in chunk_list(pmc_ids):\n",
    "        paper_idx_to_date.update(PMCIDsToYear(chunk))\n",
    "    return paper_idx_to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8cc659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recency(known, response,verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate the recency of each chemical in the response.\n",
    "    Look on the known edge, and find the earliest support for the edge using the TM PMID API\n",
    "    see\n",
    "     https://github.com/UCDenver-ccp/DocumentMetadataAPI/blob/main/README.md\n",
    "     \n",
    "    The general flow of this code is as follows. \n",
    "    1) Parse the TRAPI and find the relevant disease curies and list of drug curies from the response.\n",
    "    2) Normalize the list of drug curies.\n",
    "    3) Go to the ROBOKOPKG, ask it specifically for the TextMineKP edges between the Disease and Drug nodes.\n",
    "    4) Go to Pubmed, get the year of publication for every paper we found in step 3.\n",
    "    5) Break the results up into each drug-disease pair, and report them back.\n",
    "    \n",
    "    Step 3 and 4 are complicated by two factors. One is that it is substantially easier to query ROBOKOPKG and\n",
    "    the Pubmed API with bulk queries then individually. It would produce a better logical flow of the code to\n",
    "    go through every individual disease-drug pair, send a query to ROBOKOP to get a list of publications, then send\n",
    "    a query to Pubmed asking for the year each publication was published. Unfortunately, that takes prohibitively \n",
    "    long to run. The second issue is normalization of drugs; I don't want to drop any identifier information, so part \n",
    "    of the code is juggling the unnormalized and normalized drug identifiers.\n",
    "\n",
    "    \"\"\"\n",
    "    #Get the drug identifier from the TRAPI Query.\n",
    "    disease_idx = getQueryNode(response)[0]\n",
    "    if(verbose):print(\"We found this TRAPI had disease identifier:\",disease_idx)\n",
    "    #Get the list of all drug identifiers from the TRAPI query. This list\n",
    "    # is not necessarily normalized to the specifications of node normalizer.\n",
    "    unnormalized_drug_list = list(drug_idx_generator(response))\n",
    "    if(verbose):print(\"We found this TRAPI had the following numbers of drug identifers:\",len(unnormalized_drug_list))\n",
    "    #Go to ROBOKOPKG, query each disease-drug pair; find those with an edge from TextMiningKP.\n",
    "    # For those pairs with edges from TextMiningKP, return those publications as both a dictionary\n",
    "    # and as a large set of all identifiers we came across (we need this set of all identifiers\n",
    "    # to make querying Pubmed simplier in our next step).\n",
    "    pubs_for_pairs, all_pubs = getPublistForPairs(disease_idx,unnormalized_drug_list)\n",
    "    if(verbose):print(\"We found this TRAPI had the following numbers of publications from TextMiningKP:\",len(all_pubs))\n",
    "\n",
    "    # Generate a dictonary for the earliest year of publication for each paper \n",
    "    # identifer we found in our ROBOKOP query.\n",
    "    pub_to_date = getDatesFromPaperIdentifiers(all_pubs)\n",
    "    if(verbose):print(\"We have finished querying Pubmed and PubmedCentral for publication dates.\")\n",
    "    recency = {}\n",
    "    \n",
    "    for drug_idx in unnormalized_drug_list:        \n",
    "        pub_list = pubs_for_pairs[(disease_idx,drug_idx)]\n",
    "        if(len(pub_list)!=0): earliest_year = min([pub_to_date[paper_idx] for paper_idx in pub_list])\n",
    "        else: earliest_year = None\n",
    "        if(verbose):\n",
    "            print(disease_idx,drug_idx)\n",
    "            print(\"List of pubs\",pub_list)\n",
    "            print(earliest_year)\n",
    "            print(\"----------\")\n",
    "        recency[drug_idx] = earliest_year\n",
    "        \n",
    "    return recency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44efead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = run_score(\"debec37a-a281-47a5-a3d6-dc31206c571f\")\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daec9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load( open( \"save.p\", \"rb\" ) )\n",
    "for ars, response in results.items():\n",
    "    known, unknown = find_known_results(response)\n",
    "#        nn_distance = calculate_nn_distance(known, unknown, response)\n",
    "    recency = calculate_recency(known, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcadc5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQueryNode(json_data):\n",
    "    query_node = json_data['message']['query_graph']['nodes']\n",
    "\n",
    "    if 'on' in query_node:\n",
    "    #Weird gene query\n",
    "        if query_node['on'].get('categories',[]) == ['biolink:Gene']:\n",
    "            query_idx = \"biolink:Gene\"\n",
    "            disease = False\n",
    "        elif('ids' in query_node['on']):\n",
    "            query_idx = query_node['on']['ids'][0]\n",
    "            disease = True\n",
    "        else:\n",
    "            query_idx = \"N/A\"\n",
    "            disease = False\n",
    "    elif 'n0' in query_node:\n",
    "        if('ids' in query_node['n0']):\n",
    "            query_idx = query_node['n0']['ids'][0]\n",
    "            disease = True\n",
    "        elif('id' in query_node['n1']):\n",
    "            query_idx = query_node['n1']['id']\n",
    "            disease = True\n",
    "        else:\n",
    "            query_idx = \"N/A\"\n",
    "            disease = True\n",
    "    elif 'disease' in query_node:\n",
    "        query_idx = query_node['disease']['ids'][0]\n",
    "        disease = True\n",
    "    #Could not find any node\n",
    "    else:\n",
    "        query_idx = \"N/A\"\n",
    "        disease=False\n",
    "    return (query_idx,disease)\n",
    "            \n",
    "\n",
    "def drug_idx_generator(json_data):\n",
    "    for result in json_data['message'].get('results',[]):\n",
    "        node_bindings = result['node_bindings']\n",
    "        if 'sn' in node_bindings:\n",
    "            drug_idx = node_bindings['sn'][0]['id']\n",
    "        elif 'n1' in node_bindings:\n",
    "            drug_idx = node_bindings['n1'][0]['id']\n",
    "        elif 'drug' in node_bindings:\n",
    "            drug_idx = node_bindings['drug'][0]['id']\n",
    "        elif \"chemical\" in node_bindings:\n",
    "            drug_idx = node_bindings['chemical'][0]['id']\n",
    "        else:\n",
    "            print(node_bindings)\n",
    "            raise Exception(\"Could not get Drug\")\n",
    "        \n",
    "        #NORMALIZE DRUG IDENTIFIER\n",
    "        #drug_idx = m[drug_idx]\n",
    "        yield drug_idx\n",
    "    return\n",
    "    '''        if drug_idx not in output_data[disease_idx]:\n",
    "            output_data[disease_idx][drug_idx] = [disease_idx, drug_idx, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "        #Noticed some Aragorn queries did not have scores in the results\n",
    "        if 'score' not in result:\n",
    "            return \"error\"\n",
    "        score = result['score']\n",
    "        if 'normalized_score' in result:\n",
    "            normalized_score = result['normalized_score']\n",
    "        else:\n",
    "            normalized_score = 0\n",
    "        output_data[disease_idx][drug_idx][ara_indexing[ara]] = score\n",
    "        output_data[disease_idx][drug_idx][ara_indexing[ara] + 1] = normalized_score\n",
    "    return \"success\"'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d05dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "getQueryNode(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b753f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f022f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = py2neo.Graph(host=\"robokopkg.renci.org\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f92ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#def normalDrugIdentifiersGenerator(drug_list):\n",
    "    \n",
    "\n",
    "def queryROBOKOPForPMIDsDiseaseDrug(disease_idx,drug_idx):\n",
    "    graph = py2neo.Graph(host=\"robokopkg.renci.org\")\n",
    "    query_template = 'MATCH (d:`biolink:Disease`)-[r]-(c:`biolink:ChemicalEntity`) WHERE d.id=$disease_idx AND c.id=$drug_idx AND r.`biolink:primary_knowledge_source`=\"infores:textminingkp\" RETURN r.publications'\n",
    "    pubs_for_res=[]\n",
    "    query_res = graph.run(query_template,parameters={\"disease_idx\":disease_idx, \"drug_idx\":drug_idx})\n",
    "    for message in query_res:\n",
    "#        for publication in message['r.publications']:\n",
    "        pubs_for_res.extend(message['r.publications'])\n",
    "    return pubs_for_res\n",
    "\n",
    "\n",
    "    #    break\n",
    "    \n",
    "def getEarliestPublicationDateForPairs(disease_idx,drug_identifier_list,verbose=False):\n",
    "    \n",
    "    pubs_for_pairs, all_pubs = getPublistForPairs(disease_idx,drug_identifier_list)\n",
    "    pub_to_date = getDatesFromPMIDs(all_pubs)\n",
    "    for drug_idx in drug_identifier_list:\n",
    "        pub_list = pubs_for_pairs[(disease_idx,drug_idx)]\n",
    "        if(len(pub_list)!=0):\n",
    "            earliest_year = min([pub_to_date[paper_idx] for paper_idx in pub_list])\n",
    "        else:\n",
    "            earliest_year = None\n",
    "#        date = getEarlyDate(pubs_for_res)\n",
    "        if(verbose):\n",
    "            print(disease_idx,drug_key)\n",
    "            print(\"List of pubs\",pub_list)\n",
    "            print(earliest_year)\n",
    "            print(\"----------\")\n",
    "        yield (disease_idx,drug_key,earliest_year)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "#        date = getEarlyDate(pubs_for_res)\n",
    "#        if(verbose):\n",
    "#            print(disease_idx,drug_key)\n",
    "#            print(\"List of pubs\",pubs_for_res)\n",
    "#            print(date)\n",
    "#            print(\"----------\")\n",
    "#        yield (disease_idx,drug_key,date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322387a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryROBOKOPForPMIDsDiseaseDrug(\"MONDO:0005148\",\"CHEBI:32677\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryROBOKOPForPMIDsDiseaseDrugList(\"MONDO:0005148\",[\"CHEBI:32677\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_score('c01a5839-975f-4352-8aa3-5490d93adfc6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f753ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubs_for_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_res['r.publications']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,740,250):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chunk_list(l):\n",
    "    for i in range(0,len(l),250):\n",
    "        lower = i\n",
    "        upper = min(i+250,len(l))\n",
    "        yield l[lower:upper]\n",
    "\n",
    "def getDatesFromPaperIdentifiers(paper_ids):\n",
    "    paper_idx_to_date = {}\n",
    "    pubmed_ids = [x for x in paper_ids if \"PMC\" not in x]\n",
    "    #Break our queries into chunks of length 250.\n",
    "    for chunk in chunk_list(pubmed_ids):\n",
    "        paper_idx_to_date.update(PMIDsToYear(chunk))\n",
    "    \n",
    "    pmc_ids = [x for x in paper_ids if \"PMC\" in x]\n",
    "    for chunk in chunk_list(pmc_ids):\n",
    "        paper_idx_to_date.update(PMCIDsToYear(chunk))\n",
    "    return paper_idx_to_date\n",
    "    \n",
    "def getEarlyDate(pmids_all):\n",
    "    if(len(pmids_all)==0):return None\n",
    "    print(\"starting func\")\n",
    "    date_list = getDatesFromListPMIDs(pmids_all)\n",
    "    print('v2',date_list)\n",
    "    if(len(date_list)==0):return None\n",
    "    \n",
    "    return min([int(x) for x in date_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a298648",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids = ['PMC6182468', 'PMID:21079458', 'PMC5740191', 'PMC5740191']\n",
    "getDatesFromPMIDs(pmids)\n",
    "test_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=\" + ','.join(pmids)\n",
    "    #print(test_url)\n",
    "    #There's some weird issue with querying pubmed to quickly, easier just to have it take a pause.\n",
    "time.sleep(0.2)\n",
    "r = requests.get(test_url)\n",
    "s = r.text.encode('utf-8')\n",
    "print('query2',len(s))\n",
    "if(len(s)>250):\n",
    "    tree = etree.fromstring(s, parser=utf8_parser)\n",
    "    tree.xpath(\"//year/text()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e090f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"PMC6182468\",\"PMC5740191\",\"PMC5740191\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f885cb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids = ['PMC3321664', 'PMID:15596736', 'PMID:16533138', 'PMC3136450', 'PMID:15652415', 'PMC4164261', 'PMC3497294', 'PMC3498184', 'PMC3497350', 'PMC3521715']\n",
    "#print(getDatesFromListPMIDs(pmids))\n",
    "d = getDatesFromPMIDs(pmids)\n",
    "#print(getEarlyDate(pmids))\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree\n",
    "for article_ele in tree.xpath(\"//PubmedArticle\"):\n",
    "    pm_idx = \"PMID:\" + article_ele.xpath(\"./MedlineCitation/PMID/text()\")[0]\n",
    "    print(pm_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# <code>Python\n",
    "# \" \" \"\n",
    "#    Two functions to check the ARS's URL (https://ars-prod.transltr.io/ars/api/messages/) and then to fetch the EPC attributes      \n",
    "#    @author: Salman Zarrini\n",
    "# \" \" \"\n",
    "# </code>\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "#-------------\n",
    "def checking_extracting_attributes_1(attribute_type_id_list, pk):\n",
    "    \"\"\"\n",
    "    This function get EPC attributes from the ARS webpage using a knonw PK corresponding to a query.\n",
    "    \n",
    "    Args:\n",
    "        attribute_type_id_list (list):  attribute_type_id of interest as a list\n",
    "        pk (string): PK of an invoked query by a user\n",
    "        \n",
    "    Returns:\n",
    "        return_type: The function return two outputs, one is the total time taken to check and extract requeired\n",
    "        information from ARS. Another one is the extracted information as a Pandas DataFrame.\n",
    "        \n",
    "    Examples:\n",
    "        Ex_1:\n",
    "        attribute_type_id_list = ['biolink:publications', 'biolink:Publication', 'biolink:publication']\n",
    "        pk = 'c01a5839-975f-4352-8aa3-5490d93adfc6'\n",
    "        \n",
    "        Ex_2:\n",
    "        attribute_type_id_list = ['biolink:FDA_approval_status']\n",
    "        pk = 'c01a5839-975f-4352-8aa3-5490d93adfc6'\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    total_checking_time = 0\n",
    "    \n",
    "    \n",
    "    start_time_check = time.time()\n",
    "    \n",
    "    url = \"https://ars-prod.transltr.io/ars/api/messages/\"+pk\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    data_retrieved = response.json()\n",
    "    \n",
    "    pks_list = []\n",
    "    edges_list = []\n",
    "    all_attributes_list = []\n",
    "    value_list = []\n",
    "    \n",
    "    if ('message' in data_retrieved['fields']['data']) and ('knowledge_graph' in data_retrieved['fields']['data']['message']):\n",
    "        edges = data_retrieved['fields']['data']['message']['knowledge_graph']['edges']\n",
    "        # Going through each edge and inspecting its attributes.\n",
    "        for edge in edges.keys():\n",
    "            pk_edge_attr = edges[edge]['attributes']\n",
    "            for attr_numb in range(len(pk_edge_attr)):\n",
    "                # Checking if the 'attribute_type_id' is FDA approval\n",
    "                if pk_edge_attr[attr_numb]['attribute_type_id'] in attribute_type_id_list:\n",
    "                            \n",
    "                        end_time_check = time.time()\n",
    "                        pk_time_checking = end_time_check - start_time_check\n",
    "                        total_checking_time += pk_time_checking\n",
    "                        \n",
    "                        pks_list.append(pk)\n",
    "                        edges_list.append(edge)\n",
    "                        all_attributes_list.append(pk_edge_attr)\n",
    "                        value_list.append(pk_edge_attr[attr_numb]['value'])\n",
    "    temp = {\n",
    "        'pks': pks_list,\n",
    "        'edges': edges_list,\n",
    "        'all_attrs': all_attributes_list,\n",
    "        'values': value_list\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(temp)\n",
    "    \n",
    "    return f'Total time of checking attribute for each query: {total_checking_time}', df\n",
    "#-------------\n",
    "\n",
    "\n",
    "def checking_extracting_attributes_2(attribute_type_id_list):\n",
    "    \"\"\"\n",
    "    This function first extract the PK of the latest query and get the EPC attributes corresponding\n",
    "    the extracted PKs from the ARS webpage.\n",
    "    \n",
    "    Args:\n",
    "        attribute_type_id_list (list):  attribute_type_id of interest as a list\n",
    "        \n",
    "    Returns:\n",
    "        return_type: The function return two outputs, one is the total time taken to check and extract requeired\n",
    "        information from ARS. Another one is the extracted information as a Pandas DataFrame.\n",
    "        \n",
    "    Examples:\n",
    "        Ex_1:\n",
    "        attribute_type_id_list = ['biolink:publications', 'biolink:Publication', 'biolink:publication']\n",
    "        \n",
    "        Ex_2:\n",
    "        attribute_type_id_list = ['biolink:FDA_approval_status']\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    total_checking_time = 0\n",
    "    total_extracting_time = 0\n",
    "    \n",
    "    start_time_check = time.time()\n",
    "    \n",
    "    url = \"https://ars-prod.transltr.io/ars/api/messages/\"\n",
    "    response = requests.get(uurl)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        data_retrieved = response.json()\n",
    "    \n",
    "        # Extract the PK values from the data\n",
    "        pks_retrieved = [item['pk'] for item in data_retrieved]\n",
    "    \n",
    "        pks_list = []\n",
    "        edges_list = []\n",
    "        all_attributes_list = []\n",
    "        value_list = []\n",
    "    \n",
    "        for idx in range(len(data_retrieved)):\n",
    "            # Making sure that the query's response has 'message' and then 'knowledge_graph' as keys, respectively\n",
    "            if ('message' in data_retrieved[idx]['fields']['data']) and ('knowledge_graph' in data_retrieved[idx]['fields']['data']['message']):\n",
    "                edges = data_retrieved[idx]['fields']['data']['message']['knowledge_graph']['edges']\n",
    "                # Going through each edge and inspecting its attributes.\n",
    "                for edge in edges.keys():\n",
    "                    pk_edge_attr = edges[edge]['attributes']\n",
    "                    for attr_numb in range(len(pk_edge_attr)):\n",
    "                        # Checking if the 'attribute_type_id' is FDA approval\n",
    "                        if pk_edge_attr[attr_numb]['attribute_type_id'] in attribute_type_id_list:\n",
    "                            \n",
    "                            end_time_check = time.time()\n",
    "                            pk_time_checking = end_time_check - start_time_check\n",
    "                            total_checking_time += pk_time_checking\n",
    "                        \n",
    "                            pks_list.append(pks_retrieved[idx])\n",
    "                            edges_list.append(edge)\n",
    "                            all_attributes_list.append(pk_edge_attr)\n",
    "                            value_list.append(pk_edge_attr[attr_numb]['value'])\n",
    "        temp = {\n",
    "            'pks_': pks_list,\n",
    "            'edgs_': edges_list,\n",
    "            'all_attrs_': all_attributes_list,\n",
    "            'value_': value_list\n",
    "        }\n",
    "        df = pd.DataFrame(temp)\n",
    "        \n",
    "    else:\n",
    "        print('Failed to retrieve data. Status code:', response.status_code)  \n",
    "    \n",
    "    return f'Total time of checking attribute for each query: {total_checking_time}', df\n",
    "\n",
    "#-------------\n",
    "\n",
    "#Some examples PK from dump database to be used in the checking_extracting_attributes_1(attribute_type_id_list, pk) function:\n",
    "\n",
    "pk_dump_publication = [\n",
    " 'c01a5839-975f-4352-8aa3-5490d93adfc6',\n",
    " '7771c619-0211-4241-8284-2b1ecaad2585',\n",
    " '6cc89d3a-762f-41a6-9fd3-212e7dc71045',\n",
    " '70732f65-14bb-43be-84c3-499c13f04808',\n",
    " '1817194d-facc-414e-9e0d-df58c9d1fb88',\n",
    " '3f374b0b-9be3-4643-bc3d-94f3a704b5d2',\n",
    " '73bf8218-0364-48b9-af53-963779255778',\n",
    " '8a882a78-90e3-4451-bfbf-1903a4fc670d',\n",
    " '2791e7cf-a6eb-49bb-b82f-01102295a4b2',\n",
    " '9f0089ef-b45e-4440-ae52-31329d029d41',\n",
    " '9fef2e41-6dc5-453b-9a9f-e6e870e866a0',\n",
    " '08977acc-380a-477c-801a-4cfea879052b',\n",
    " 'a8287166-e73a-47ae-a4fc-c32f903edcdb',\n",
    " '167a254a-8434-4dce-be31-6b401881ee78',\n",
    " 'b7ce6ccf-246e-4378-9bbb-8d5701a95ff9',\n",
    " 'f4afdf27-22fe-4cf9-9cd8-388cff57e933',\n",
    " '84522585-cf0b-4dbd-a752-93f2f8edf0a1',\n",
    " '902a3a45-5b35-4abb-a931-40bf77ce8ea4',\n",
    " 'afe3458f-0b2f-4327-80fc-feddbf9373c4',\n",
    " '118e7dd5-ae29-4085-8a5b-1fa0e9b26f0e']\n",
    "\n",
    "\n",
    "pk_dump_fda = [\n",
    " '6cc89d3a-762f-41a6-9fd3-212e7dc71045',\n",
    " '70732f65-14bb-43be-84c3-499c13f04808',\n",
    " '8a882a78-90e3-4451-bfbf-1903a4fc670d',\n",
    " 'a8287166-e73a-47ae-a4fc-c32f903edcdb',\n",
    " 'b7ce6ccf-246e-4378-9bbb-8d5701a95ff9',\n",
    " '118e7dd5-ae29-4085-8a5b-1fa0e9b26f0e',\n",
    " '50b80303-d8db-4195-9979-44340fee8ab8',\n",
    " 'f3cb207c-3c5f-4a2a-80b3-ac60774500d0',\n",
    " 'a4b08898-b517-4557-9cb4-121de17365e5',\n",
    " '94e876ef-836f-4334-8ca0-1f88b541e23e',\n",
    " 'b2c53e95-6947-4365-8f77-9d344c8a6b97',\n",
    " 'a8b55833-ce1a-4f12-9a7e-ff465b5df902',\n",
    " 'a11a837d-0557-42db-9316-c9fc5d19b27e',\n",
    " '937b6eb2-0201-4bab-b2d3-dff7e7d1b59d',\n",
    " 'f23fba18-503e-49d9-8ca9-03022d98be45',\n",
    " 'd287a80d-8ef9-4270-932c-55d67c46ba70',\n",
    " '7cc631c4-b5c6-475e-9353-093b8e29fde7',\n",
    " '1c12b8a4-9894-4b3f-b547-4efe7f38a580',\n",
    " '803ec852-d7b8-4c4b-a7d5-7d7b01618a95',\n",
    " '1e5ca2cb-93af-4fe8-87d2-48069532b9b5']\n",
    "\n",
    "pk_dump_not_pub_fda = [\n",
    " 'c01a5839-975f-4352-8aa3-5490d93adfc6',\n",
    " '70732f65-14bb-43be-84c3-499c13f04808',\n",
    " '2edbca56-3759-4bae-a0a1-c404ff15c189',\n",
    " 'a347d888-bd3e-46d0-ab15-74241956fb08',\n",
    " '9f0089ef-b45e-4440-ae52-31329d029d41',\n",
    " 'a8287166-e73a-47ae-a4fc-c32f903edcdb',\n",
    " '2119f139-38b6-444f-99fa-905650777fe8',\n",
    " '1e5371c6-4219-4b42-b441-cc5ac86b767e',\n",
    " 'afe3458f-0b2f-4327-80fc-feddbf9373c4',\n",
    " 'f3cb207c-3c5f-4a2a-80b3-ac60774500d0',\n",
    " 'f0b876f7-d37f-4160-8c93-d841fc64b787',\n",
    " 'a4b08898-b517-4557-9cb4-121de17365e5',\n",
    " 'b2c53e95-6947-4365-8f77-9d344c8a6b97',\n",
    " '4a32b5d3-584b-44ad-9eb5-788c5b4b3f84',\n",
    " 'c341af23-23ea-44db-9fcf-ff64a30c30a2',\n",
    " '0cfd2aa2-41a1-43bd-bb1a-ae4f267e1089',\n",
    " '576b1692-f2c6-4c0b-a82d-a0c668d6ec72',\n",
    " 'a8b55833-ce1a-4f12-9a7e-ff465b5df902',\n",
    " 'b34267a3-788a-49ef-8d20-8cfda13a97f5',\n",
    " 'f23fba18-503e-49d9-8ca9-03022d98be45']\n",
    "\n",
    "#----------------\n",
    "\n",
    "# An example for extracting the attribute_type_id of publication in att_list for three example PK and concatenate them together:\n",
    "\n",
    "att_list = ['biolink:publications', 'biolink:Publication', 'biolink:publication']\n",
    "\n",
    "df_publication = pd.DataFrame(columns= ['pks', 'edges', 'all_attrs', 'values'])\n",
    "\n",
    "for pk in pk_dump_publication[:1]:\n",
    "    tot_time, df = checking_extracting_attributes_1(att_list, pk)\n",
    "    df_publication = pd.concat([df_publication, df], ignore_index=True)\n",
    "\n",
    "\n",
    "#print(df_publication)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
